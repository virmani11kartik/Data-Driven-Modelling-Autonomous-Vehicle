{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Script: Gaussian Process Regression for Learning Dynamics\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from tqdm import trange\n",
    "import scipy.io\n",
    "\n",
    "# Load simulation data\n",
    "mat_file_path = 'vehicle_dynamics_data.mat'\n",
    "mat_data = scipy.io.loadmat(mat_file_path)\n",
    "results = mat_data['results'][0, 0]  # Extract the outer structure\n",
    "\n",
    "# Terrain keys\n",
    "terrains = [\"Snow\", \"Mud\", \"SteepSlope\"]\n",
    "\n",
    "# Initialize storage for training data\n",
    "data_X, data_Y = [], []\n",
    "\n",
    "for terrain in terrains:\n",
    "    # Extract terrain-specific data\n",
    "    terrain_data = results[terrain][0, 0]\n",
    "    x = terrain_data['x'][:, 0]    # 200x1 -> flatten to 1D\n",
    "    y = terrain_data['y'][:, 0]\n",
    "    theta = terrain_data['theta'][:, 0]\n",
    "    v = terrain_data['v'][:, 0]\n",
    "    delta = terrain_data['delta'][0, :]  # 1x200 -> flatten to 1D\n",
    "    a = terrain_data['a'][0, :]\n",
    "\n",
    "    # Calculate dynamics errors (ground truth - nominal)\n",
    "    dx = np.diff(x)\n",
    "    dy = np.diff(y)\n",
    "    dv = np.diff(v)\n",
    "\n",
    "    # Feature matrix: [v, delta, a]\n",
    "    inputs = np.vstack((v[:-1], delta[:-1], a[:-1])).T\n",
    "    data_X.append(inputs)\n",
    "\n",
    "    # Targets: [dx, dy, dv]\n",
    "    targets = np.vstack((dx, dy, dv)).T\n",
    "    data_Y.append(targets)\n",
    "\n",
    "# Combine data from all terrains\n",
    "data_X = np.vstack(data_X)\n",
    "data_Y = np.vstack(data_Y)\n",
    "\n",
    "# Check shapes for debugging\n",
    "print(f\"Final data_X shape: {data_X.shape}\")\n",
    "print(f\"Final data_Y shape: {data_Y.shape}\")\n",
    "\n",
    "\n",
    "# Train Gaussian Process Regressors (one for each output)\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n",
    "gpr_models = []\n",
    "\n",
    "for i in trange(data_Y.shape[1]):\n",
    "    gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=1e-2)\n",
    "    gpr.fit(data_X, data_Y[:, i])\n",
    "    gpr_models.append(gpr)\n",
    "\n",
    "# Validation and Visualization\n",
    "for i, gpr in enumerate(gpr_models):\n",
    "    # Predict on training data for simplicity (or use a test split)\n",
    "    y_pred, sigma = gpr.predict(data_X, return_std=True)\n",
    "\n",
    "    # Plot predictions vs actual\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(data_Y[:, i], label=\"True\", alpha=0.7)\n",
    "    plt.plot(y_pred, label=\"Predicted\", linestyle=\"--\")\n",
    "    plt.fill_between(\n",
    "        np.arange(len(y_pred)), y_pred - 1.96 * sigma, y_pred + 1.96 * sigma,\n",
    "        alpha=0.2, label=\"Confidence Interval\"\n",
    "    )\n",
    "    plt.title(f\"Output Dimension {i + 1}: Prediction vs Actual\")\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"Dynamics Error\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Save models and evaluation metrics\n",
    "import joblib\n",
    "\n",
    "for i, gpr in enumerate(gpr_models):\n",
    "    joblib.dump(gpr, f\"gpr_model_output_{i}.joblib\")\n",
    "\n",
    "print(\"GPR training and validation complete. Models saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "from jax import vmap, jit, vjp, random\n",
    "from jax.scipy.linalg import cholesky, solve_triangular\n",
    "\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "from pyDOE import lhs\n",
    "from functools import partial\n",
    "from tqdm import trange\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "onp.random.seed(1234)\n",
    "# Neural network \\(\\phi(x)\\)\n",
    "def neural_net_phi(nn_params, x):\n",
    "    W1 = (np.array([nn_params[0], nn_params[1]]).reshape(-1,1)).T\n",
    "    b1 = np.array([nn_params[2], nn_params[3]])\n",
    "    W2 = np.array([nn_params[4], nn_params[5]]).reshape(-1,1)\n",
    "    b2 = np.array(nn_params[6])\n",
    "    h = np.tanh(np.dot(x, W1) + b1)  # Hidden layer\n",
    "    return np.dot(h, W2) + b2  # Output layer\n",
    "# A wrapper to call SciPy's L-BFGS-B optimizer\n",
    "def minimize_lbfgs(objective, x0, bnds = None):\n",
    "    result = minimize(objective, x0, jac=True,\n",
    "                      method='L-BFGS-B', bounds = bnds,\n",
    "                      callback=None)\n",
    "    return result.x, result.fun\n",
    "# A vectorized RBF kernel function\n",
    "def RBF(x1, x2, params):\n",
    "    output_scale = params[0]\n",
    "    lengthscales = params[1:]\n",
    "    diffs = np.expand_dims(x1 / lengthscales, 1) - \\\n",
    "            np.expand_dims(x2 / lengthscales, 0)\n",
    "    r2 = np.sum(diffs**2, axis=2)\n",
    "    return output_scale * np.exp(-0.5 * r2)\n",
    "\n",
    "mu = lambda x: np.cos(4.0*np.pi*x)\n",
    "def RBF_with_phi(nn_params, x1, x2, theta):\n",
    "    phi_x1 = vmap(lambda x: neural_net_phi(nn_params, x))(x1)\n",
    "    phi_x2 = vmap(lambda x: neural_net_phi(nn_params, x))(x2)\n",
    "    return RBF(phi_x1, phi_x2, theta)  # RBF kernel on transformed inputs\n",
    "# A minimal Gaussian process class\n",
    "class GPRegression:\n",
    "    # Initialize the class\n",
    "    def __init__(self, kernel_fn = RBF_with_phi):\n",
    "        self.kernel = kernel_fn\n",
    "\n",
    "    def random_init_GP(self, rng_key, dim):\n",
    "        # GPR Parameters\n",
    "        logsigma_f = np.log(50.0*random.uniform(rng_key, (1,)))\n",
    "        loglength  = np.log(random.uniform(rng_key, (dim,)) + 1e-8)\n",
    "        logsigma_n = np.array([-4.0]) + random.normal(rng_key, (1,))\n",
    "        # Neural Network Parameters\n",
    "        input_dim=1 \n",
    "        hidden_dim=2 # ONE HIDDEN LAYER WITH 2 NEURONS\n",
    "        output_dim=1\n",
    "        k1, k2 = random.split(rng_key)\n",
    "        W1 = random.normal(k1, (input_dim, hidden_dim)) * 0.1\n",
    "        W1_1 = np.array(W1[0,0]).reshape(-1)  \n",
    "        W1_2 = np.array(W1[0,1]).reshape(-1) \n",
    "        b1 = random.normal(k1, (hidden_dim,)) * 0.1\n",
    "        b1_1 = b1[0].reshape(-1)\n",
    "        b1_2 = b1[1].reshape(-1)\n",
    "        W2 = random.normal(k2, (hidden_dim, output_dim)) * 0.1\n",
    "        W2_1 = W2[0, 0].reshape(-1) \n",
    "        W2_2 = W2[1, 0].reshape(-1)\n",
    "        b2 = random.normal(k2, (output_dim,)) * 0.1\n",
    "        b2 = b2.reshape(-1)\n",
    "        hyp = np.concatenate([W1_1,W1_2,b1_1,b1_2,W2_1,W2_2,b2,logsigma_f, loglength, logsigma_n])\n",
    "        return hyp\n",
    "\n",
    "    def compute_cholesky(self, params, batch):\n",
    "        X, _ = batch\n",
    "        N, D = X.shape\n",
    "        # Fetch params\n",
    "        sigma_n = np.exp(params[-1])\n",
    "        theta = np.exp(params[-3:-1])\n",
    "        nn_params = params[:7]\n",
    "        # Compute kernel\n",
    "        K = self.kernel(nn_params, X, X, theta) + np.eye(N)*(sigma_n + 1e-8)\n",
    "        L = cholesky(K, lower=True)\n",
    "        return L\n",
    "\n",
    "    def likelihood(self, params, batch):\n",
    "        _, y = batch\n",
    "        N = y.shape[0]\n",
    "        # Compute Cholesky\n",
    "        L = self.compute_cholesky(params, batch)\n",
    "        # Compute negative log-marginal likelihood\n",
    "        alpha = solve_triangular(L.T,solve_triangular(L, y, lower=True))\n",
    "        NLML = 0.5*np.matmul(np.transpose(y),alpha) + \\\n",
    "               np.sum(np.log(np.diag(L))) + 0.5*N*np.log(2.0*np.pi)\n",
    "        return NLML\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def likelihood_value_and_grad(self, params, batch):\n",
    "        fun = lambda params: self.likelihood(params, batch)\n",
    "        primals, f_vjp = vjp(fun, params)\n",
    "        grads = f_vjp(np.ones_like(primals))[0]\n",
    "        return primals, grads\n",
    "\n",
    "    def train(self, batch, rng_key, num_restarts = 10):\n",
    "        # Define objective that returns NumPy arrays (to be minimized with SciPy)\n",
    "        def objective(params):\n",
    "            value, grads = self.likelihood_value_and_grad(params, batch)\n",
    "            out = (onp.array(value), onp.array(grads))\n",
    "            return out\n",
    "        # Optimize with random restarts\n",
    "        params = []\n",
    "        likelihood = []\n",
    "        X, _ = batch\n",
    "        dim = X.shape[1]\n",
    "        rng_key = random.split(rng_key, num_restarts)\n",
    "        for i in trange(num_restarts):\n",
    "            init = self.random_init_GP(rng_key[i], dim)\n",
    "            p, val = minimize_lbfgs(objective, init)\n",
    "            params.append(p)\n",
    "            likelihood.append(val)\n",
    "        params = np.vstack(params)\n",
    "        likelihood = np.vstack(likelihood)\n",
    "        #### find the best likelihood (excluding any NaNs) ####\n",
    "        bestlikelihood = np.nanmin(likelihood)\n",
    "        idx_best = np.where(likelihood == bestlikelihood)\n",
    "        idx_best = idx_best[0][0]\n",
    "        best_params = params[idx_best,:]\n",
    "        return best_params\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def predict(self, params, batch, X_star):\n",
    "        X, y = batch\n",
    "        # Fetch params\n",
    "        sigma_n = np.exp(params[-1])\n",
    "        theta = np.exp(params[-3:-1])\n",
    "        nn_params = params[:7]\n",
    "        # Compute kernels\n",
    "        k_pp = self.kernel(nn_params,X_star, X_star, theta) + np.eye(X_star.shape[0])*(sigma_n + 1e-8)\n",
    "        k_pX = self.kernel(nn_params,X_star, X, theta)\n",
    "        L = self.compute_cholesky(params, batch)\n",
    "        alpha = solve_triangular(L.T,solve_triangular(L, y, lower=True))\n",
    "        beta  = solve_triangular(L.T,solve_triangular(L, k_pX.T, lower=True))\n",
    "        # Compute predictive mean, std\n",
    "        mu = np.matmul(k_pX, alpha)\n",
    "        cov = k_pp - np.matmul(k_pX, beta)\n",
    "        std = np.sqrt(np.clip(np.diag(cov), a_min=0.))\n",
    "        return mu, std\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def draw_posterior_sample(self, rng_key, params, batch, X_star):\n",
    "        X, y = batch\n",
    "        N, D = X.shape\n",
    "        # Fetch params\n",
    "        sigma_n = np.exp(params[-1])\n",
    "        theta = np.exp(params[:-1])\n",
    "        nn_params = params[:7]\n",
    "        # Compute kernels\n",
    "        k_pp = self.kernel(nn_params,X_star, X_star, theta) + np.eye(X_star.shape[0])*(sigma_n + 1e-8)\n",
    "        k_pX = self.kernel(nn_params,X_star, X, theta)\n",
    "        L = self.compute_cholesky(params, batch)\n",
    "        alpha = solve_triangular(L.T,solve_triangular(L, y, lower=True))\n",
    "        beta  = solve_triangular(L.T,solve_triangular(L, k_pX.T, lower=True))\n",
    "        # Compute predictive mean\n",
    "        mu = np.matmul(k_pX, alpha).reshape(-1)\n",
    "        cov = k_pp - np.matmul(k_pX, beta)\n",
    "        sample = random.multivariate_normal(rng_key, mu, cov)\n",
    "        return sample\n",
    "\n",
    "\n",
    "# Update kernel function in GP model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_key, noise_key = random.split(random.PRNGKey(1))\n",
    "for i in trange(data_Y.shape[1]):\n",
    "    gp_model = GPRegression(kernel_fn=RBF_with_phi)\n",
    "    X_train_nf = data_X[:,i].reshape(-1,1)\n",
    "    X_test = data_X[:,i].reshape(-1,1)\n",
    "    y_train_nf = data_Y[:,i].reshape(-1,1)\n",
    "    y_true = data_Y[:,i]\n",
    "    y_test = data_Y[:,i]\n",
    "    mu_y_nf, sigma_y_nf = y_train_nf.mean(0), y_train_nf.std(0)\n",
    "    #X_train_nf_norm = (X_train_nf - (-1)) / (1 - (-1))  # Normalize to [0, 1]\n",
    "    #y_train_nf_norm = (y_train_nf - mu_y_nf) / sigma_y_nf\n",
    "\n",
    "    # Train the GP model\n",
    "    opt_params_nf = gp_model.train((X_train_nf, y_train_nf), train_key, num_restarts=10)\n",
    "    mean_nf, std_nf = gp_model.predict(opt_params_nf, (X_train_nf, y_train_nf), X_test)\n",
    "    # Test data and exact step function\n",
    "    #X_test = np.linspace(0, 30, 597)[:, None]\n",
    "   \n",
    "    #X_test_norm = (X_test - (-1)) / (1 - (-1))  # Normalize to [0, 1]\n",
    "    #mean_nf, std_nf = mean_nf * sigma_y_nf + mu_y_nf, std_nf * sigma_y_nf\n",
    "    mean_nf = mean_nf.flatten()\n",
    "\n",
    "    # Check accuracy\n",
    "    error_nf = np.linalg.norm(mean_nf-y_test,2)/np.linalg.norm(y_test,2)\n",
    "    print(\"Relative L2 error u_nf: %e\" % (error_nf))\n",
    "\n",
    "    # Draw Posterior Samples\n",
    "    num_draws = 10\n",
    "    test_keys = random.split(random.PRNGKey(0), num_draws)\n",
    "\n",
    "    sample_fn_nf = lambda key: gp_model.draw_posterior_sample(key, opt_params_nf, (X_train_nf, y_train_nf), X_test)\n",
    "    f_samples_nf = vmap(sample_fn_nf)(test_keys)\n",
    "    f_samples_nf = f_samples_nf*sigma_y_nf + mu_y_nf\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(32, 8))\n",
    "\n",
    "    # Noise-free data\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(y_true[:], label=\"True\", alpha=0.7)\n",
    "    plt.plot(y_train_nf[:], 'kx', label=\"Training data\")\n",
    "    plt.plot(mean_nf[:], 'r--', label=\"GP Mean\")\n",
    "    #plt.fill_between((mean_nf[:] - 2 * std_nf[:]).flatten(), (mean_nf[:] + 2 * std_nf[:]).flatten(),\n",
    "    #                 color='orange', alpha=0.5, label=\"95% confidence interval\")\n",
    "    plt.fill_between(\n",
    "            np.arange(len(mean_nf)), mean_nf - 1.96 * std_nf, mean_nf + 1.96 * std_nf,\n",
    "            alpha=0.2, label=\"Confidence Interval\"\n",
    "        )\n",
    "    #for i in range(num_draws):\n",
    "    #     plt.plot(X_test, f_samples_nf[i,:], '-', color='gray', lw = 0.5)\n",
    "    plt.title(\"Noise-free data\")\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
